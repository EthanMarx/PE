First Pipeline
========

After installing the base `AMPLFI` package, you will have access to a handy command, `amplfi-init`, for initializing run directories:

```console
> poetry run amplfi-init --help

usage: amplfi-init [-h] [--mode {flow,similarity}] [--pipeline {tune,train}] [-d DIRECTORY] [--s3-bucket S3_BUCKET]

Initialize a directory with configuration files for running end-to-end amplfi training or tuning pipelines

optional arguments:
  -h, --help            Show this help message and exit.
  --mode {flow,similarity}
                        Either 'flow' or 'similarity',Whether to setup a flow or similarity training (default: flow)
  --pipeline {tune,train}
                        Either 'train' or 'tune'.Whether to setup a tune or train pipeline (default: train)
  -d DIRECTORY, --directory DIRECTORY
                        The run directory where theconfiguration files will be copied to (required, type: <class 'Path'>)
  --s3-bucket S3_BUCKET
                        (default: null)
```

For example, let's initialize a run directory at `~/amplfi/my-first-run` for training a normalizing flow

```console
poetry run amplfi-init --mode flow --pipeline train --directory ~/amplfi/flow-run
```

A `run.sh` will be created in the run directory that will look like:

```bash
#!/bin/bash
# Export environment variables
export AMPLFI_DATADIR=/home/albert.einstein/amplfi/data/
export AMPLFI_OUTDIR=/home/albert.einstein/amplfi/my-first-run/training/
export AMPLFI_CONDORDIR=/home/albert.einstein/amplfi/my-first-run/data/condor

# launch the data generation pipeline;
LAW_CONFIG_FILE=/home/albert.einstein/amplfi/my-first-run/datagen.cfg poetry run --directory /home/albert.einstein/projects/amplfi/amplfi/law law run amplfi.law.DataGeneration --workers 5

# launch training or tuning pipeline
poetry run --directory /home/albert.einstein/projects/amplfi/projects/train python /home/albert.einstein/projects/amplfi/projects/train/train/cli/flow.py fit --config cbc.yaml
```

This bash script consists of two steps:
1. Querying gravitational wave strain data using [`law`](github.com/riga/law)
2. Training a normalizing flow using Pytorch Lightning

The data querying step is controlled by the `datagen.cfg` file configuration. This will query segments of science-mode strain data,
and save them in the directory specified by the `AMPLFI_DATADIR` environment variable. This step uses htcondor for parallelization,
and will save any condor log files to `AMPLFI_CONDORDIR`.

Once data querying is complete, training will begin. Training configuration is controlled by the `train.yaml` file. It's imporant to get familiar with the training parameters, but the defaults should suffice for your first run. The training job will look in `AMPLFI_DATADIR` for strain data, and will save checkpoints and other training artifacts in `AMPLFI_OUTDIR`.

Once training has complete, sample corner plots, skymaps, and probability-probability plots (PP-plots) can be generated by running the `test` subcommand. Remember to pass your trained model weights, which can be found at the $AMPLFI_OUTDIR directory. In this case,
we pass the weights corresponding to the best validation score, which are automatically saved at `$AMPLFI_OUTDIR/train_logs/best.ckpt`

```console
poetry run --directory /home/albert.einstein/projects/amplfi/projects/train python /home/albert.einstein/projects/amplfi/projects/train/train/cli/flow.py test --config /path/to/config.yaml --model.checkpoint=$AMPLFI_OUTDIR/train_logs/best.ckpt
```
